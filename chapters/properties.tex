\documentclass[output=paper
	        ,collection
	        ,collectionchapter
 	        ,biblatex
                ,babelshorthands
                ,newtxmath
                ,draftmode
                ,colorlinks, citecolor=brown
]{langscibook}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[1]
}{}

\title{Basic properties and elements} 
\author{%
 Anne Abeillé\affiliation{Université Paris Diderot}%
 \lastand Robert D. Borsley\affiliation{University of Essex}%
}
% \chapterDOI{} %will be filled in at production

%\epigram{Change epigram in chapters/01.tex or remove it there }

\abstract{Head-driven Phrase Structure Grammar (HPSG) is a declarative and monostratal version of generative grammar, in which linguistic expressions have a single relatively simple constituent structure. It seeks to develop detailed formal analyses using a system of types, features, and constraints. Constraints on types of \emph{lexical-sign} are central to the lexicon of a language and constraints on types of \emph{phrase} are at the heart of the syntax, and both lexical and phrasal types include semantic and phonological information. Different versions of the framework have been developed, including versions in which constituent order is a reflection not of constituent structure but of a separate system of order domains and the Sign-Based Construction Grammar version, which make a fundamental distinction between signs of various kinds and the constructions which license them.%
%
\footnote{We are grateful to Stefan Müller, Jean-Pierre Koenig, and Frank Richter for many helpful comments on earlier versions of this chapter. We alone are responsible for what appears here.}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\label{chapter-basic-properties}\label{chap-properties}

\inlinetodostefan{footnote from the title had to be moved here bc it created compilation errors}
\inlinetodostefan{Fix the label for page-hfp \label{page-hfp}}


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Introduction} 
Head-driven Phrase Structure Grammar (HPSG) dates back to early 1985 when Carl Pollard presented his Lectures on HPSG. It was often seen in the early days as a revised version of the earlier Generalized Phrase Structure Grammar (GPSG) framework \citep{GKPS85a}, but it was also influenced by Categorial Grammar, and, as \citet[1]{ps} emphasized, by other frameworks like Lexical-Functional Grammar (LFG) \citep{Bresnan82a-ed}, as well. Naturally it has changed in various ways over the decades. This is discussed in much more detail in the next chapter (Flickinger et al., this volume, chapter~\ref{chap-evolution}), but it makes sense here to distinguish three versions of HPSG. Firstly, there is what might be called early HPSG, the framework presented in \citet{ps} and \citet{ps2}%
%
\footnote{As discussed in Richter, this volume, chapter~\ref{chap-formal-background}, the approaches that are developed in these two books have rather different formal foundations. However, they propose broadly similar syntactic analyses, and for this reason it seems reasonable to group them together as early HPSG.}.
%
This has most of the properties of more recent versions but only exploits the analytic potential of type hierarchies to a limited degree \citep{Flickinger87,FPW85a}. Next there is what is sometimes called Constructional HPSG, the framework adopted in \citet{Sag97a,GSag2000a-u}, and much other work. Unlike earlier work this uses a rich hierarchy of phrase-types. This is why it is called constructional%
%
\footnote{As discussed below, HPSG has always assumed a rich hierarchy of lexical types. One might argue, therefore, that it has always been constructional.}.
%
Finally, in the 2000s, Sag developed a version of HPSG called \emph{Sign-Based Construction Grammar} (SBCG) \citep{Sag2012a}. The fact that this approach has a new name suggests that it is very different from earlier work, but probably most researchers in HPSG would see it as a version of HPSG, and it was identified as such in \citet[486]{Sag2010b}. Its central feature is the special status it assigns to constructions. In earlier work they are just types of sign, but for SBSG signs and constructions are quite different objects. In spite of this difference, most analyses in Constructional HPSG could probably be translated into SBCG and vice versa. In this chapter we will concentrate on the ideas of Constructional HPSG, which is probably the version of the framework that has been most widely assumed. We will comment briefly on SBCG in the final section.

\inlinetodostefan{Add sec refs}
The chapter is organized as follows. In section~2, we set out the properties that characterize the approach and the assumptions it makes about the nature of linguistic analyses and the conduct of linguistic research. Then, in section~3, we consider the main elements of HPSG analyses: types, features, and constraints. In section~4 we look more closely at the HPSG approach to the lexicon, and in section~5, we outline the basics of the HPSG approach to syntax. In section~6, we look at some further syntactic structures, and in section~7, we consider some further topics, including SBCG. Finally, in section~8, we summarize the chapter.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Properties}
Perhaps the first thing to say about HPSG is that it is a form of generative grammar in the sense of \citet{Chomsky57a}. This means that it seeks to develop precise and explicit analyses of grammatical phenomena. But unlike many versions of generative grammar, it is a declarative or constraint-based approach to grammar, belonging to what \citet{PullumScholz2001} call ‘Model Theoretic syntax’. As such, it assumes that a linguistic analysis involves a set of constraints to which linguistic objects must conform, and that a linguistic object is well-formed if and only if it conforms to all relevant constraints.%
%
\footnote{In most HPSG work, all constraints are equal. Hence, there is no possibility as there is in Optimality Theory of violating one if it is the only way to satisfy another more important one \citep{Malouf2003a}. However, see \citealt{MK2000a-cr,OFTM2004a-u} for an HPSG parser with probabilities or weighted constraints.}
%
This includes linguistic objects of all kinds – words, phrases, phonological segments, etc. There are no procedures constructing representations such as the phrase structure and transformational rules of classical transformational grammar or the Merge and Agree operations of Minimalism. Of course, speakers and hearers do construct representations and must have procedures that enable them to do so, but this is a matter of performance, and there is no need to think that the knowledge that is used in performance has a procedural character. Rather, the fact that it used in both production and comprehension (and other activities, e.g. translation) suggests that it should be neutral between the two and hence declarative. For further discussion of the issues, see e.g. \citet{PullumScholz2001,Postal2003a} and \citet{SW2011a,SW2015a}.

HPSG is also a monostratal approach, which assumes that linguistic expressions have a single constituent structure. This makes it quite different from transformational grammar, in which an expression can have a number of constituent structures. It means among other things that there is no possibility of saying that an expression occupies one position at one level of structure and another at another. Hence, HPSG has nothing like the movement processes of transformational grammar. The relations that are attributed to movement in transformational work are captured by constraints that require certain features to have the same value. For example, as discussed in section~4\todostefan{add sec ref}, a raising sentence is one with a verb which has the same value for the feature \textsc{subj(ect)} as its complement and hence combines with whatever kind of subject its complement requires.

HPSG is sometimes described as a concrete approach to syntax. This description refers not only to the fact that it assumes a single constituent structure but also to the fact that this structure is relatively simple, especially compared with the structures that are postulated within Minimalism. Unlike Minimalism, HPSG does not assume that all branching is binary. This inevitably leads to simpler flatter structures. Also unlike Minimalism, it makes limited use of phonologically empty elements. For example, it is not assumed, as in Minimalism, that because some clauses contain a complementizer they all do, an empty one if not an overt one. Similarly, it is not assumed that because some languages like English have determiners, they all do, overt or covert. It is also not generally assumed that null subject sentences, such as (\ref{ex:prop1b}) from Polish, have a phonologically empty subject in their constituent structure. Thus, the constituent structure of the two following sentences are quite different, even if their semantics are similar:

\ea\label{ex:prop1}
	\ea\label{ex:prop1a}
	I read a book.
	\ex\label{ex:prop1b}
	\gll Czytałem książkę.\\
	read\textsc{.pst.1sg} book\textsc{.acc}\\
	\glt ‘I read a book.’
	\z
\z

It is also assumed in much HPSG work that there are no phonologically empty elements in the constituent structure of an unbounded dependency construction such as the following:

\ea\label{ex:prop2}
What did you say?
\z

On this view, the verb \emph{say} in (\ref{ex:prop2}) does not have an empty complement. There is, however, some debate here (\citealp{SF94a,Mueller2004e}; Borsley \& Crysmann, this volume, chapter~\ref{sec:UDC:MoreOnGaps}).

{\color{red}
A further important feature of HPSG is a rejection of the Chomskyan idea that grammatical phenomena can be divided into a core, which merits serious investigation, and a periphery, which can be safely ignored.%
%
\footnote{This is not to deny that some constructions are more canonical and more frequent in use than others and that this may be important in various ways.}
%
This means that it is not only concerned with such ‘core’ phenomena as \emph{wh}-interrogatives, relative clauses, and passives but also with more ‘peripheral’ phenomena such as the following:
}

\ea\label{ex:prop3}
	\ea\label{ex:prop3a}
	It’s amazing the people you see here.
	\ex\label{ex:prop3b}
	The more I read, the more I understand.
	\ex\label{ex:prop3c}
	Chris lied his way into the meeting.
	\z
\z

These exemplify the nominal extraposition construction \citep{MichaelisLambrecht1996}, the comparative correlative construction \citep{Abeille2006a,AB2008a-u,Borsley2011a-u}, and the \emph{X’s Way} construction \citep[7.4]{KF99a,Sag2012a}. As we will see, HPSG is an approach which is able to accommodate broad linguistic generalizations and highly idiosyncratic facts and everything in between.%
%
\footnote{Idioms have also been an important focus of research in HPSG. See e.g. \citealt{RS2009a,KM2019a}, and Sailer, this volume, chapter~\ref{chap-idioms}.}
%

Another notable feature of the framework since the earliest work is a concern with semantics as well as syntax. More generally, it does not try to reduce either semantics or morphology to syntax (see Crysmann, this volume, chapter~\ref{chap-morphology}, Koenig and Richter, this volume, chapter~\ref{chap-semantics}). We will comment further on this in the following sections.

We turn now to some assumptions which are more about the conduct of linguistic research than the nature of linguistic analyses. Firstly, HPSG emphasizes the importance of firm empirical foundations and of detailed formal analyses of the kind advocated by Chomsky in \emph{Syntactic Structures} \citep{Chomsky57a}. Whereas transformational work typically offers sketches of analyses which might be fleshed out one day, HPSG commonly provides detailed analyses which can be set out in an appendix. A notable example is \citet{GSag2000a-u}, which sets out its analysis of English interrogatives in a 50 page appendix. Arguably, one can only be fully confident that a complex analysis works if it is incorporated into a computer implementation. Hence, computer implementations of HPSG analyses are also quite common (see e.g. \citealp{Babel,MuellerCoreGram,Copestake2002a,BDFPS2010a-u,Bender2016}, and Bender and Emerson, this volume, chapter~\ref{chap-cl}).

Another property of the framework is a rejection of abstract analyses with tenuous links to the observable data. As we noted above, phonologically empty elements are only assumed if there is compelling evidence for them.%
%
\footnote{There may be compelling evidence for some empty elements in some languages. Thus, \citet[section~8]{Borsley2009a-u}Borsley argues that Welsh has phonologically empty pronouns. For general discussion of empty elements, see \citet[chapter~19.2]{MuellerGT-Eng1-linked}.}
%
Similarly, overt elements are only assumed to have properties for which there is clear evidence. For example, words are only assumed to have case or agreement features if there is some concrete morphological evidence for them, as in Polish, illustrated in (\ref{ex:prop1b}). This feature of HPSG stems largely from considerations about acquisition (\citealt[chapter~19]{MuellerGT-Eng1-linked}; Ginzburg this volume, Borsley and Müller, this volume, chapter~\ref{sec-acquisition-minimalism}). Every element or property which is postulated for which there is no clear evidence in the data increases the complexity of the acquisition task and hence necessitates more complex innate machinery. This suggests that such elements and properties should be avoided as much as possible. It has important implications both for the analysis of individual languages and for how differences between languages are viewed.
	
A related property of the framework is a rejection of the idea that it is reasonable to assume that a language has some element or property if some other languages do. Many languages have case and many languages have agreement, but for HPSG it does not follow that they all do. As \citet[25]{MuellerCoreGram} puts it, “Grammars should be motivated on a language-specific basis.” Does this mean that other languages are irrelevant when one investigates a specific language? Clearly not. As Müller also puts it, “In situations where more than one analysis would be compatible with a given dataset for language X, the evidence from language Y with similar constructs is most welcome and can be used as evidence in favour of one of the two analyses for language X.” (\citeyear[43]{MuellerCoreGram})


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Elements}

For HPSG, a linguistic analysis is a system of types (or sorts), features, and constraints. Types provide a complex classification of linguistic objects, features identify their basic properties, and constraints impose further restrictions. In this section, we will explain these three elements. We note at the outset that HPSG distinguishes between the linguistic objects (lexemes, words phrases, etc.) and descriptions of such objects. Linguistic objects must have all relevant properties of their description and cannot be underspecified in any way. Descriptions in contrast can be underspecified and in fact always are.

There are many different kinds of types, but particularly important is the type sign and its various subtypes. For \citet[19]{GSag2000a-u}, this type has the subtypes lexical-sign and phrase, and lexical-sign has the subtypes lexeme and word. (Types are written in lower case italics.) Thus, we have the following type hierarchy in Figure~\ref{fig:prop1}.


\begin{figure}[h!]
\begin{forest}
[\emph{sign}
	[\emph{lexical-sign}
		[\emph{lexeme}]
		[\emph{word}]
	]
	[\emph{phrase}]
]
\end{forest}

\caption{A hierarchy of types of signs}\label{fig:prop1}
\end{figure}


\emph{Lexeme}, \emph{word} and \emph{phrase} have a complex system of subtypes. The type \emph{lexical-sign}, its subtypes, and the constraints on them are central to the lexicon of a language, while the type \emph{phrase}, its subtypes, and the constraints on them are at the heart of the syntax. In both cases, complex hierarchies mean that the framework is able to deal with broad, general facts, very idiosyncratic facts, and everything in between. We will say more about this below.

Signs are obviously complex objects with (at least) phonological, syntactic and semantic properties. Hence, the type \emph{sign} must have features that encode these properties. For much work in HPSG, phonological properties are encoded as the value of a feature \textsc{phon(ology)}, whose value is a list of objects of type \emph{phon}, while syntactic and semantic properties are grouped together as the value of a feature \textsc{synsem}, whose value is an object of type \emph{synsem}. (Feature or attributes are written in small caps.) A type has certain features associated with it, and each feature has a value of some kind. A bundle of features can be represented by an attribute"=value"=matrix (AVM) with the type name at the top on the left hand side and the features below followed by their values. Thus, signs can be described as follows:


\ea\label{ex:prop4}
\avmtmp{
	[\type*{sign}
	phon & \upshape list !(\textit{phon})!\\
	synsem & synsem ]
}
\z
\todostefan{“list” is in textit in \ref{ex:prop6} tho}


The descriptions of specific signs will obviously have specific values for the two features. For example, we might have the following simplified AVM for the phrase \emph{the cat}:

\ea\label{ex:prop5}
\avmtmp{
	[\type*{phrase}
	phon & <\upshape the,cat>\\
	synsem & \upshape NP ]
}
\z


Here, following a widespread practice, we use standard orthography instead of real \emph{phon} objects%
%
\footnote{See Bird \& Klein 1999, Höhle 2018%
	%
	\inlinetodostefan{both not given in references, are they not supposed to appear there? What section are the others referring to?}
	%
	and de Kuthy (this volume, section~7), Abeillé and Chaves (this volume, section~7).}%
%
, and we use the traditional label NP as an abbreviation for the relevant \emph{synsem} object. We will say more about \emph{synsem} objects shortly. First, however, we must say something about phrases.

A central feature of phrases is that they have internal constituents. More precisely, they have daughters, i.e. immediate constituents, one of which may be the head. This information is encoded by further features, for \citet[29]{GSag2000a-u} the features \textsc{daughters} (\textsc{dtrs}) and \textsc{head-daughter} (\textsc{hd-dtr}). The value of the latter is a \emph{sign}, and the value of the former is a list of \emph{signs}, which includes the value of the latter.%
%
\footnote{Some HPSG work, e.g. \citet{Sag97a} has a \textsc{head-daughter} feature and a \textsc{non-head-daughters} feature, and the value of the former is not part of the value of the latter. The sign that is the value of \textsc{hd-dtr} can be a word or a phrase. Within Minimalism, the term ‘head’ is only applied to words. On this usage, the value of \textsc{hd-dtr} is either the head or a phrase containing the head. But there are good reasons for not adopting this usage, for example the fact that the head can be an unheaded phrase for example a coordination (see Abeillé and Chaves, \inlinetodostefan{“chapter” has been lowercase up until now} Chapter~\ref{chap-coordination} of this volume). So we will say that the value of \textsc{hd-dtr} is the head. See \citet[30]{Jackendoff77a} for an early discussion of the term.}
%
{\color{red}
Thus, phrases take the form in (\ref{ex:prop6a}), and headed-phrases the form in (\ref{ex:prop6b}):
}

\begin{multicols}{2}
\ea\label{ex:prop6}
	\ea\label{ex:prop6a}
	\avmtmp{
		[\type*{phrase}
		phon & list!(phon)!\\
		synsem & synsem\\
		dtrs & list!(sign)! ]
	}

\columnbreak
	
	\ex\label{ex:prop6b}
	\avmtmp{
		[\type*{headed-phrase}
		phon & list!(phon)!\\
		synsem & synsem\\
		dtrs & list!(sign)!\\
		hd-dtr & sign ]
	}
	\z
\z
\end{multicols}

To take a concrete example the phrase \emph{the cat} might have the fuller AVM in (\ref{ex:prop7}).

\ea\label{ex:prop7}
\avmtmp{
	[\type*{phrase}
	phon & <\textnormal{the,cat}>\\
	synsem & \textnormal{NP}\\
	dtrs &	<[phon & <\textnormal{the}>\\
			synsem & \textnormal{Det}],
%			
			\1 [phon & <\textnormal{cat}>\\
			synsem & \textnormal{N} ] >\\
	hd-dtr & \1 ]
}
\z

Here the two instances of the tag \avmtmp{\1} indicate that the \emph{synsem} object which is the second member of the \textsc{dtrs} list is also the value of \textsc{hd-dtr}. Thus, the word \emph{cat} is the head of the phrase \emph{the cat}. An object occupying more than one position in a representation, either as a feature value or as part of a feature value, for example \avmtmp{\1} in (\ref{ex:prop7}), is known as re-entrancy or structure-sharing. As we will see below, it is a pervasive feature of HPSG.

Most HPSG work on morphology has assumed a realizational approach, in which there are no morphemes (see Crysmann, this volume, chapter~\ref{chap-morphology}). Hence, words do not have internal structures in the way that phrases do. However, it is widely assumed that lexemes and words that are derived through a lexical rule have the lexeme from which they are derived as a daughter (see Koenig 1999 and below section~4.3).\todostefan{not in references; add sec ref} Hence, the \textsc{dtrs} feature is relevant to words as well as phrases.

AVMs like (\ref{ex:prop7}) can be quite hard to look at. Hence, it is common to use traditional tree diagrams instead. Thus, we might have a tree-like representation in Figure~\ref{fig:prop2} instead of (\ref{ex:prop7}). But one should bear in mind that AVMs correspond to (rooted) graphs and provide richer descriptions than traditional phrase structure trees, with richer node labels and edge labels, and with shared feature values between nodes. Thus, at each node, all kinds of information are available, not just syntax but also semantics and phonology.%
%
\footnote{This differs from Lexical Functional Grammar for instance, which distributes the information between different kinds of structures (see Wechsler and Asudeh, this volume, chapter~\ref{chap-lfg}).}
%

\begin{figure}
\begin{forest}
[NP
	[Det
		[the]
	]
	[N, edge label={node[midway,right]{\textsc{hd-dtr}}}
		[cat]
	]
]
\end{forest}
	
\caption{A simple tree for \emph{the cat}}\label{fig:prop2}
\end{figure}

If the head is either obvious or unimportant, the \textsc{hd-dtr} annotation might be omitted. This is a convenient informal notation, but it is important to remember that it is just that and has no status within the theory.

We return now to \emph{synsem} objects. Standardly these have two features: \textsc{local}, whose value is a \emph{local} object and \textsc{nonlocal}, which we will deal with in section~5.\todostefan{add sec ref} A \emph{local} object has the features \textsc{cat(egory)} and \textsc{cont(ent)}, whose values are objects of type \emph{category} and \emph{content}, respectively, and the feature \textsc{context}.%
%
\footnote{Words also have a \textsc{morph} attribute that we ignore here (see Crysmann, this volume, chapter~22).\inlinetodostefan{ch 22 isnt by Crysmann!?}}
%
In much work, a \emph{category} object has the features, HEAD, \textsc{subj} and \textsc{comp(lement)s}. \textsc{head} takes as its value a \emph{part-of-speech} object, while \textsc{subj} and \textsc{comps} have a list of \emph{synsem} objects as their value. The former indicates what sort of subject a sign requires and the latter indicates what complements it takes. In both cases the value is the empty list if nothing is required.  It is generally assumed that the \textsc{subj} list never has more than one member. \textsc{subj} and \textsc{comps} are often called \emph{valence} features. Thus, the following AVM provides a fuller representation of signs:

\ea\label{ex:prop8}
\avmtmp{
	[\type*{sign}
	phon & list!(phon)!\\
	synsem &	[\type*{synsem}
				local &	[\type*{local}
						category &	[\type*{category}
									head & part-of-speech\\
									subj & list!(synsem)!\\
									comps & list!(synsem)!]\\
						content\\
						context]\\
				nonlocal\ldots]
	]
}
\z

The type \emph{part-of-speech} has subtypes such as \emph{noun}, \emph{verb}, and \emph{adjective}. In other words, we have a type hierarchy of the following form:

\begin{figure}
\begin{forest}
[\emph{part-of-speech}
	[\emph{noun}]
	[\emph{verb}]
	[\emph{adjective}]
	[\ldots]
]
\end{forest}	
\caption{A hierarchy for part of speech}\label{fig:prop3}
\end{figure}

The type hierarchy can be viewed as an ontology of possible objects in the language. A particular word or phrase must instantiate one of the maximal (most specific) types and have the properties specified for it and all its supertypes.%
%
\footnote{AVMs associated with types used to be combined by type unification \citep{ps}. See Richter, this volume, chapter~\ref{chap-introduction}.}
%
We might have a \emph{synsem} object of the following form for the phrase \emph{the cat}:

\ea\label{ex:prop9}
\avmtmp{
	[\type*{synsem}
	local &	[\type*{local}
			category &	[\type*{category}
						head & noun\\
						subj & <>\\
						comps & <>]\\
			content\\
			context]\\
	nonlocal\ldots]
}
\z

This ignores a number of matters including the value of \textsc{content, context}, and \textsc{nonlocal}. It also ignores the fact that the type \emph{noun} will have certain features, for example \textsc{case}, but it highlights some important aspects of HPSG analyses. Notice that (\ref{ex:prop9}) is compatible with the \textsc{synsem} feature in (\ref{ex:prop8}): it contains more specific information, such as \textsc{head} noun, but no conflicting information: $\langle \rangle$ is the empty list, and is compatible with \emph{list(synsem)}.

Rather different from most of the features mentioned above are fairly traditional features like \textsc{person, number, gender}, and \textsc{case}. In most HPSG work, these have as their value an atomic type, a type with no features. A simple treatment of person might have the types \emph{first, second}, and \emph{third}, and a simple treatment of number the types \emph{sing(ular)} and \emph{plur(al)}.%
%
\footnote{In practice a more complex system of values may well be appropriate \citep{Flickinger2000a}.}
%
There are also Boolean features with + and – as their values. An example is \textsc{aux} used to distinguish auxiliary verbs ([\textsc{aux}~+]) from non-auxiliary verbs ([\textsc{aux}~–]).%
%
\footnote{In some recent work, e.g. \citet[157--162]{Sag2012a}, \citet{Sag2020a}, the feature is used to distinguish positions that only allow an auxiliary from positions that allow any verb. Within this approach auxiliaries (except support do) are unspecified for \textsc{aux} since they may appear in both [\textsc{aux}~+] and [\textsc{aux}~–] constructions. Non-auxiliary verbs are [\textsc{aux}~–], see Abeillé, this volume, chapter~\ref{chap-control-raising}.}
%

As the preceding makes clear, features in HPSG can have a number of kinds of value. They may have an atomic type (\textsc{person, number, gender, case, aux}), a feature structure (\textsc{synsem, local, category}, etc.), or a list of some kind (\textsc{subj, comps}).%
%
\footnote{A list can be analysed as a type of feature structure with the features \textsc{first} and \textsc{rest}, where the value of \textsc{first} is the first element of the list.}
%
As we will see in section~5\todostefan{add sec ref}, HPSG also assumes features with a set as their value.

The \textsc{content} feature, whose value is a \emph{content} object, highlights the importance of semantics within HPSG. But what exactly is a \emph{content} object? Different views of semantics have been taken within the HPSG literature. Much HPSG work has assumed some version of Situation Semantics \citep{BP83a}. But some work has employed so-called Minimal Recursion Semantics \citep{CFPS2005a}, while some others use Logical Resource Semantics \citep{RichterandSailer2001}\todosatz{ref from hpsg-handbook-bibliography.bib!}. \citet[501]{Sag2010b} adopts a conventional, Montague-style possible-worlds semantics in his analysis of English filler-gap constructions and SBCG (section~7.2)\todostefan{add secref} has generally employed a version of Frame Semantics. See Koenig and Richter, this volume, chapter~\ref{chap-semantics} for a discussion of the issues.

Finally, the \textsc{context} feature is used for information structure, deixis, and more generally pragmatics (see de Kuthy, this volume, chapter~\ref{chap-information-structure}).

We will say more about types and features in the following sections. We turn now to constraints. These are implicational statements, saying that if a linguistic object has some property or properties then it must have some other property or properties. They take the following form:%
%
\footnote{The double-shafted arrow ($\Rightarrow$) is used in constraints, and a single shafted arrow |$\rightarrow$ in lexical rules.}
%

\ea\label{ex:prop10}
X $\Rightarrow$ Y
\z

Commonly X is a type and Y a feature description, and this is the case in all the constraints that we discuss below. However, X may also be a feature description with or without an associated type. This is necessary, for example, in the constraints that constitute binding theory. See Müller and Branco, this volume, chapter~\ref{chap-binding}. Here is a very simple constraint:

\ea\label{ex:prop11}
\emph{phrase} $\Rightarrow$ [\textsc{comps} $\langle \rangle$]
\z

This says that a phrase has the empty list ($\langle \rangle$) as the value of the \textsc{comps} feature, which means that it does not require any complements.%
%
\footnote{The constraint in (\ref{ex:prop11}) is plausible for English, but it is too strong for some languages, especially for languages with complex predicates or partial VPs (see Godard and Samvelian, this volume, chapter~1),\inlinetodostefan{what ch is this actually referring to??} \textcolor{red}{and also for SOV languages if they are analysed in terms of binary branching (see Müller, this volume, chapter~\ref{chap-order}).}}
%
As we will see below, most constraints are more complex than (\ref{ex:prop11}) and impose a number of restrictions on certain objects. For this reason, one might speak of a set of constraints. However, we will continue to use the term constraint for objects of the form in (\ref{ex:prop10}), no matter how many restrictions are imposed. Particularly important are constraints dealing with internal structure of various types of phrase. We will consider some constraints of this kind in section~5.\todostefan{add secref}

In most HPSG work, some shortcuts are used to abbreviate a feature path, for example in (\ref{ex:prop11}), \textsc{comps} stands for \textsc{synsem|loc|cat|comps}. We use this practice in the rest of the chapter and \textcolor{red}{it is used} throughout the Handbook.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{The lexicon}

As noted above, the type \emph{lexical-sign}, its subtypes, and the constraints on them are central to the lexicon of a language and the words it licenses.%
%
\footnote{Other types of constraint are relevant to the form of lexemes and words, e.g. constraints on \emph{synsem} objects and constraints on \textsc{phon} values. These are also relevant to the form of phrases.}
%
Lexical rules are also important. Some of the earliest work in HPSG focused on the organization of the lexicon and the question of how lexical generalizations can be captured, and detailed proposals have been developed.%
%
\footnote{The lexicon is more important in HPSG than in some other constructional approaches, e.g. that of \citet{Goldberg95a,Goldberg2006a}. See \citet{MWArgSt} and Müller, this volume, chapter~\ref{chap-cxg} for discussion.}
%

%%%%%%%%%%%%%%%%%%%%
\subsection{Lexemes and words}

In some frameworks, the lexicon contains not lexemes but morphemes, i.e. roots and affixes of various kinds. But most work in HPSG has assumed a realizational approach to morphology. Within this approach, there are no morphemes, just lexemes and the words that realize them, and affixes are just bits of phonology realizing certain morphosyntactic features \citep{Stump2001a-u-kopiert,Anderson92a-u}. One consequence of this is that HPSG has no syntactic elements like the T(ense) and Num(ber) functional heads of Minimalism, which are mainly realized by affixes. See Crysmann, this volume, chapter~\ref{chap-morphology} and Davis and Koenig, this volume, chapter~\ref{chap-lexicon} for discussion.

Probably the most important properties of any lexeme are its part of speech and its combinatorial properties. As we saw in the last section, the \textsc{head} feature encodes part of speech information while the \textsc{subj} and \textsc{comps} features encode combinatorial information. As we also noted in the last section, \textsc{head} takes as its value a \emph{part-of-speech} object and the type \emph{part-of-speech} has subtypes such as \emph{noun}, \emph{verb}, and \emph{adjective}. At least some of the subtypes have certain features. For example, in many languages the type noun has the feature \textsc{case} with values like \emph{nom(inative), acc(usative)}, and \emph{gen(itive)}. Thus, nominative pronouns like \emph{I} might have a \emph{part-of-speech} of the form in (\ref{ex:prop12}) as its \textsc{head} value.

\ea\label{ex:prop12}
\avmtmp{
	[\type*{noun}
	case & nom]
}
\z

Similarly, in many languages the type verb has the feature \textsc{vform} with values like \emph{fin(ite)} and \emph{inf(initive)}. Thus, the \emph{part-of-speech} of the word form \emph{be} might be (\ref{ex:prop13}).

\ea\label{ex:prop13}
\avmtmp{
	[\type*{verb}
	vform & inf]
}
\z

In much the same way, the type \emph{adjective} will have a feature distinguishing between positive, comparative, and superlative forms, in English and many other languages.

We must now say more about combinatorial properties. In much HPSG work it is assumed that \textsc{subj} and \textsc{comps} encode what might be regarded as superficial combinatorial information and more basic combinatorial information is encoded by a feature \textsc{arg(ument)-st(ructure)}.%
%
\footnote{\textsc{arg-st} is also crucial for binding theory, which takes the form of a number of constraints on \textsc{arg-st} lists. See Müller and Branco, this volume, chapter~\ref{chap-binding}.}
%
Normally the value of \textsc{arg-st} of a word is the concatenation of the values of \textsc{subj} and \textsc{comps}, using $\oplus$ for list concatenation. In other words, we normally have the following situation (notice the use of re-entrancy or structure-sharing):

\ea\label{ex:prop14}
\avmtmp{
	[subj & \1\\
	comps & \2\\
	arg-st & \1 $\oplus$ \2]
}
\z

As noted earlier, it is generally assumed that the \textsc{subj} list never has more than one member. The appropriate features for the word \emph{read} in (\ref{ex:prop1a}) for example would include the following, where the tags identify not lists but list members:

\ea\label{ex:prop15}
\avmtmp{
	[subj & <\1>\\
	comps & <\2>\\
	arg-st & <\1 \upshape NP, \2 NP>]
}
\z

Under some circumstances, however, we have something different. For example, it has been proposed, e.g. in \citet[65]{ManningandSag1998},\todosatz{ref from hpsg-handbook-bib} that null subject sentences have an element representing the understood subject in the \textsc{arg-st} list of the main verb but nothing in the \textsc{subj} list. Thus, the verb \emph{czytałem} in (\ref{ex:prop1b}), repeated here as (\ref{ex:prop16}), has the features in (\ref{ex:prop17}).

\ea\label{ex:prop16}
\gll Czytałem książkę.\\
write.\textsc{pst.1sg} book.\textsc{acc}\\
\glt ‘I read a book.’

\ex\label{ex:prop17}
\avmtmp{
	[subj & <>\\
	comps & <\1>\\
	arg-st & <\upshape NP, \1NP>]
}
\z

A similar analysis is widely assumed for unbounded dependency gaps. On this analysis, the verb \emph{say} in (\ref{ex:prop2}), repeated here as (\ref{ex:prop18}), has the features in (\ref{ex:prop19}).

\ea\label{ex:prop18}
What did you say?

\ex\label{ex:prop19}
\avmtmp{
	[subj & <\1 \upshape NP>\\
	comps & <>\\
	arg-st & <\1 NP, NP>]
}
\z

It is also assumed that the arguments that are realised \textcolor{red}{as pronominal affixes (traditionally known as clitics in Romance languages)} are absent from \textsc{comps} lists \citep{MS97a-u,monachesi05},\todosatz{Monachesi from hpsg-hb.bib} and other differences between \textsc{subj} and \textsc{comps} and \textsc{arg-st} have been proposed for other languages (see \citealt{ManningandSag1998}, Wechsler, Koenig and Davies,\todosatz{from hpsg-bib} this volume, chapter~\ref{chap-arg-st} for discussion. In much work, the relation \textsc{arg-st} and \textsc{subj} and \textsc{comps} is regulated by a constraint called the Argument Realisation Principle (ARP). The following is a simplified version of the constraint proposed in \citet[171]{GSag2000a-u} (see also \citealt[12]{BMS2001a}):

\ea\label{ex:prop20}
\emph{word} $\Rightarrow$
\avmtmp{
	[subj & \1\\
	comps & \2 $\ominus$ \textnormal{list}!(non-canonical)!\\
	arg-st & \1 $\oplus$ \2]
}
\z












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\caption{Frequencies of word classes}
\label{tab:1:frequencies}
 \begin{tabular}{lllll} % add l for every additional column or remove as necessary
  \lsptoprule
            & nouns & verbs & adjectives & adverbs\\ %table header
  \midrule
  absolute  &   12 &    34  &    23     & 13\\
  relative  &   3.1 &   8.9 &    5.7    & 3.2\\
  \lspbottomrule
 \end{tabular}
\end{table}

\citep{Chomsky57a}.

\citet{Meier2017}
\ea\label{ex:1:descartes}
\langinfo{Latin}{}{personal knowledge}\\
\gll cogit-o ergo sum \\
think-1{\SG}.{\PRS}.{\IND} hence exist.1{\SG}.{\PRS}.{\IND}\\
\glt `I think therefore I am'
\z


\is{prolegomena}
Sed cursus \footnote{eros condimentum mi consectetur, ac consectetur} sapien pulvinar. Sed consequat, magna\footnote{eu scelerisque laoreet, ante erat tristique justo, nec cursus eros diam eu nisl. Vestibulum non arcu tellus}. Nunc dignissim tristique massa ut gravida. Nullam auctor orci gravida tellus egestas, vitae pharetra nisl porttitor. Pellentesque turpis nulla, venenatis id porttitor non, volutpat ut leo. Etiam hendrerit scelerisque luctus. Nam sed egestas est. Suspendisse potenti. Nunc vestibulum nec odio non laoreet. Proin lacinia nulla lectus, eu vehicula erat vehicula sed. 

\section*{Abbreviations}
\begin{tabularx}{.45\textwidth}{lX}
\textsc{cop} & copula\\ 
\textsc{fv} & final vowel\\
\end{tabularx}
\begin{tabularx}{.45\textwidth}{lX}
\textsc{neg} & negation\\ 
\textsc{sm} & subject marker\\
\end{tabularx}


\section*{Acknowledgements}

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
}
\end{document}
