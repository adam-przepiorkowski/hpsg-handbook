\documentclass[output=paper
	        ,collection
	        ,collectionchapter
 	        ,biblatex
                ,babelshorthands
                ,newtxmath
                ,draftmode
                ,colorlinks, citecolor=brown
]{langscibook}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \input{../localpackages}
  \input{../localcommands}
  \input{../locallangscifixes.tex}

  \togglepaper[1]
}{}

\title{Basic properties and elements} 
\author{%
 Anne Abeillé\affiliation{Université Paris Diderot}%
 \lastand Robert D. Borsley\affiliation{University of Essex}%
}
% \chapterDOI{} %will be filled in at production

%\epigram{Change epigram in chapters/01.tex or remove it there }

\abstract{Head-driven Phrase Structure Grammar (HPSG) is a declarative and monostratal version of generative grammar, in which linguistic expressions have a single relatively simple constituent structure. It seeks to develop detailed formal analyses using a system of types, features, and constraints. Constraints on types of \emph{lexical-sign} are central to the lexicon of a language and constraints on types of \emph{phrase} are at the heart of the syntax, and both lexical and phrasal types include semantic and phonological information. Different versions of the framework have been developed, including versions in which constituent order is a reflection not of constituent structure but of a separate system of order domains and the Sign-Based Construction Grammar version, which make a fundamental distinction between signs of various kinds and the constructions which license them.%
%
\footnote{We are grateful to Stefan Müller, Jean-Pierre Koenig, and Frank Richter for many helpful comments on earlier versions of this chapter. We alone are responsible for what appears here.}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\label{chapter-basic-properties}\label{chap-properties}

\inlinetodostefan{footnote from the title had to be moved here bc it created compilation errors}
\inlinetodostefan{Fix the label for page-hfp \label{page-hfp}}


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Introduction} 
Head-driven Phrase Structure Grammar (HPSG) dates back to early 1985 when Carl Pollard presented his Lectures on HPSG. It was often seen in the early days as a revised version of the earlier Generalized Phrase Structure Grammar (GPSG) framework \citep{GKPS85a}, but it was also influenced by Categorial Grammar, and, as \citet[1]{ps} emphasized, by other frameworks like Lexical-Functional Grammar (LFG) \citep{Bresnan82a-ed}, as well. Naturally it has changed in various ways over the decades. This is discussed in much more detail in the next chapter (Flickinger et al., this volume, chapter~\ref{chap-evolution}), but it makes sense here to distinguish three versions of HPSG. Firstly, there is what might be called early HPSG, the framework presented in \citet{ps} and \citet{ps2}%
%
\footnote{As discussed in Richter, this volume, chapter~\ref{chap-formal-background}, the approaches that are developed in these two books have rather different formal foundations. However, they propose broadly similar syntactic analyses, and for this reason it seems reasonable to group them together as early HPSG.}.
%
This has most of the properties of more recent versions but only exploits the analytic potential of type hierarchies to a limited degree \citep{Flickinger87,FPW85a}. Next there is what is sometimes called Constructional HPSG, the framework adopted in \citet{Sag97a,GSag2000a-u}, and much other work. Unlike earlier work this uses a rich hierarchy of phrase-types. This is why it is called constructional%
%
\footnote{As discussed below, HPSG has always assumed a rich hierarchy of lexical types. One might argue, therefore, that it has always been constructional.}.
%
Finally, in the 2000s, Sag developed a version of HPSG called \emph{Sign-Based Construction Grammar} (SBCG) \citep{Sag2012a}. The fact that this approach has a new name suggests that it is very different from earlier work, but probably most researchers in HPSG would see it as a version of HPSG, and it was identified as such in \citet[486]{Sag2010b}. Its central feature is the special status it assigns to constructions. In earlier work they are just types of sign, but for SBSG signs and constructions are quite different objects. In spite of this difference, most analyses in Constructional HPSG could probably be translated into SBCG and vice versa. In this chapter we will concentrate on the ideas of Constructional HPSG, which is probably the version of the framework that has been most widely assumed. We will comment briefly on SBCG in the final section.

\inlinetodostefan{Add sec refs}
The chapter is organized as follows. In section~2, we set out the properties that characterize the approach and the assumptions it makes about the nature of linguistic analyses and the conduct of linguistic research. Then, in section~3, we consider the main elements of HPSG analyses: types, features, and constraints. In section~4 we look more closely at the HPSG approach to the lexicon, and in section~5, we outline the basics of the HPSG approach to syntax. In section~6, we look at some further syntactic structures, and in section~7, we consider some further topics, including SBCG. Finally, in section~8, we summarize the chapter.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Properties}
Perhaps the first thing to say about HPSG is that it is a form of generative grammar in the sense of \citet{Chomsky57a}. This means that it seeks to develop precise and explicit analyses of grammatical phenomena. But unlike many versions of generative grammar, it is a declarative or constraint-based approach to grammar, belonging to what \citet{PullumScholz2001} call ‘Model Theoretic syntax’. As such, it assumes that a linguistic analysis involves a set of constraints to which linguistic objects must conform, and that a linguistic object is well-formed if and only if it conforms to all relevant constraints.%
%
\footnote{In most HPSG work, all constraints are equal. Hence, there is no possibility as there is in Optimality Theory of violating one if it is the only way to satisfy another more important one \citep{Malouf2003a}. However, see \citealt{MK2000a-cr,OFTM2004a-u} for an HPSG parser with probabilities or weighted constraints.}
%
This includes linguistic objects of all kinds – words, phrases, phonological segments, etc. There are no procedures constructing representations such as the phrase structure and transformational rules of classical transformational grammar or the Merge and Agree operations of Minimalism. Of course, speakers and hearers do construct representations and must have procedures that enable them to do so, but this is a matter of performance, and there is no need to think that the knowledge that is used in performance has a procedural character. Rather, the fact that it used in both production and comprehension (and other activities, e.g. translation) suggests that it should be neutral between the two and hence declarative. For further discussion of the issues, see e.g. \citet{PullumScholz2001,Postal2003a} and \citet{SW2011a,SW2015a}.

HPSG is also a monostratal approach, which assumes that linguistic expressions have a single constituent structure. This makes it quite different from transformational grammar, in which an expression can have a number of constituent structures. It means among other things that there is no possibility of saying that an expression occupies one position at one level of structure and another at another. Hence, HPSG has nothing like the movement processes of transformational grammar. The relations that are attributed to movement in transformational work are captured by constraints that require certain features to have the same value. For example, as discussed in section~4\todosatz{add sec ref}, a raising sentence is one with a verb which has the same value for the feature \textsc{subj(ect)} as its complement and hence combines with whatever kind of subject its complement requires.

HPSG is sometimes described as a concrete approach to syntax. This description refers not only to the fact that it assumes a single constituent structure but also to the fact that this structure is relatively simple, especially compared with the structures that are postulated within Minimalism. Unlike Minimalism, HPSG does not assume that all branching is binary. This inevitably leads to simpler flatter structures. Also unlike Minimalism, it makes limited use of phonologically empty elements. For example, it is not assumed, as in Minimalism, that because some clauses contain a complementizer they all do, an empty one if not an overt one. Similarly, it is not assumed that because some languages like English have determiners, they all do, overt or covert. It is also not generally assumed that null subject sentences, such as (\ref{ex:prop1b}) from Polish, have a phonologically empty subject in their constituent structure. Thus, the constituent structure of the two following sentences are quite different, even if their semantics are similar:

\ea\label{ex:prop1}
	\ea\label{ex:prop1a}
	I read a book.
	\ex\label{ex:prop1b}
	\gll Czytałem książkę.\\
	read\textsc{.pst.1sg} book\textsc{.acc}\\
	\glt ‘I read a book.’
	\z
\z

It is also assumed in much HPSG work that there are no phonologically empty elements in the constituent structure of an unbounded dependency construction such as the following:

\ea\label{ex:prop2}
What did you say?
\z

On this view, the verb \emph{say} in (\ref{ex:prop2}) does not have an empty complement. There is, however, some debate here (\citealp{SF94a,Mueller2004e}; Borsley \& Crysmann, this volume, chapter~\ref{sec:UDC:MoreOnGaps}).

{\color{red}
A further important feature of HPSG is a rejection of the Chomskyan idea that grammatical phenomena can be divided into a core, which merits serious investigation, and a periphery, which can be safely ignored.%
%
\footnote{This is not to deny that some constructions are more canonical and more frequent in use than others and that this may be important in various ways.}
%
This means that it is not only concerned with such ‘core’ phenomena as \emph{wh}-interrogatives, relative clauses, and passives but also with more ‘peripheral’ phenomena such as the following:
}

\ea\label{ex:prop3}
	\ea\label{ex:prop3a}
	It’s amazing the people you see here.
	\ex\label{ex:prop3b}
	The more I read, the more I understand.
	\ex\label{ex:prop3c}
	Chris lied his way into the meeting.
	\z
\z

These exemplify the nominal extraposition construction \citep{MichaelisLambrecht1996}, the comparative correlative construction \citep{Abeille2006a,AB2008a-u,Borsley2011a-u}, and the \emph{X’s Way} construction \citep[7.4]{KF99a,Sag2012a}. As we will see, HPSG is an approach which is able to accommodate broad linguistic generalizations and highly idiosyncratic facts and everything in between.%
%
\footnote{Idioms have also been an important focus of research in HPSG. See e.g. \citealt{RS2009a,KM2019a}, and Sailer, this volume, chapter~\ref{chap-idioms}.}
%

Another notable feature of the framework since the earliest work is a concern with semantics as well as syntax. More generally, it does not try to reduce either semantics or morphology to syntax (see Crysmann, this volume, chapter~\ref{chap-morphology}, Koenig and Richter, this volume, chapter~\ref{chap-semantics}). We will comment further on this in the following sections.

We turn now to some assumptions which are more about the conduct of linguistic research than the nature of linguistic analyses. Firstly, HPSG emphasizes the importance of firm empirical foundations and of detailed formal analyses of the kind advocated by Chomsky in \emph{Syntactic Structures} \citep{Chomsky57a}. Whereas transformational work typically offers sketches of analyses which might be fleshed out one day, HPSG commonly provides detailed analyses which can be set out in an appendix. A notable example is \citet{GSag2000a-u}, which sets out its analysis of English interrogatives in a 50 page appendix. Arguably, one can only be fully confident that a complex analysis works if it is incorporated into a computer implementation. Hence, computer implementations of HPSG analyses are also quite common (see e.g. \citealp{Babel,MuellerCoreGram,Copestake2002a,BDFPS2010a-u,Bender2016}, and Bender and Emerson, this volume, chapter~\ref{chap-cl}).

Another property of the framework is a rejection of abstract analyses with tenuous links to the observable data. As we noted above, phonologically empty elements are only assumed if there is compelling evidence for them.%
%
\footnote{There may be compelling evidence for some empty elements in some languages. Thus, \citet[section~8]{Borsley2009a-u}Borsley argues that Welsh has phonologically empty pronouns. For general discussion of empty elements, see \citet[chapter~19.2]{MuellerGT-Eng1-linked}.}
%
Similarly, overt elements are only assumed to have properties for which there is clear evidence. For example, words are only assumed to have case or agreement features if there is some concrete morphological evidence for them, as in Polish, illustrated in (\ref{ex:prop1b}). This feature of HPSG stems largely from considerations about acquisition (\citealt[chapter~19]{MuellerGT-Eng1-linked}; Ginzburg this volume, Borsley and Müller, this volume, chapter~\ref{sec-acquisition-minimalism}). Every element or property which is postulated for which there is no clear evidence in the data increases the complexity of the acquisition task and hence necessitates more complex innate machinery. This suggests that such elements and properties should be avoided as much as possible. It has important implications both for the analysis of individual languages and for how differences between languages are viewed.
	
A related property of the framework is a rejection of the idea that it is reasonable to assume that a language has some element or property if some other languages do. Many languages have case and many languages have agreement, but for HPSG it does not follow that they all do. As \citet[25]{MuellerCoreGram} puts it, “Grammars should be motivated on a language-specific basis.” Does this mean that other languages are irrelevant when one investigates a specific language? Clearly not. As Müller also puts it, “In situations where more than one analysis would be compatible with a given dataset for language X, the evidence from language Y with similar constructs is most welcome and can be used as evidence in favour of one of the two analyses for language X.” (\citeyear[43]{MuellerCoreGram})


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Elements}

For HPSG, a linguistic analysis is a system of types (or sorts), features, and constraints. Types provide a complex classification of linguistic objects, features identify their basic properties, and constraints impose further restrictions. In this section, we will explain these three elements. We note at the outset that HPSG distinguishes between the linguistic objects (lexemes, words phrases, etc.) and descriptions of such objects. Linguistic objects must have all relevant properties of their description and cannot be underspecified in any way. Descriptions in contrast can be underspecified and in fact always are.

There are many different kinds of types, but particularly important is the type sign and its various subtypes. For \citet[19]{GSag2000a-u}, this type has the subtypes lexical-sign and phrase, and lexical-sign has the subtypes lexeme and word. (Types are written in lower case italics.) Thus, we have the following type hierarchy in Figure~\ref{fig:prop1}.


\begin{figure}[h!]
	\itshape
\begin{forest}
[sign
	[lexical-sign
		[lexeme]
		[word]
	]
	[phrase]
]
\end{forest}

\caption{A hierarchy of types of signs}\label{fig:prop1}
\end{figure}


\emph{Lexeme}, \emph{word} and \emph{phrase} have a complex system of subtypes. The type \emph{lexical-sign}, its subtypes, and the constraints on them are central to the lexicon of a language, while the type \emph{phrase}, its subtypes, and the constraints on them are at the heart of the syntax. In both cases, complex hierarchies mean that the framework is able to deal with broad, general facts, very idiosyncratic facts, and everything in between. We will say more about this below.

Signs are obviously complex objects with (at least) phonological, syntactic and semantic properties. Hence, the type \emph{sign} must have features that encode these properties. For much work in HPSG, phonological properties are encoded as the value of a feature \textsc{phon(ology)}, whose value is a list of objects of type \emph{phon}, while syntactic and semantic properties are grouped together as the value of a feature \textsc{synsem}, whose value is an object of type \emph{synsem}. (Feature or attributes are written in small caps.) A type has certain features associated with it, and each feature has a value of some kind. A bundle of features can be represented by an attribute"=value"=matrix (AVM) with the type name at the top on the left hand side and the features below followed by their values. Thus, signs can be described as follows:


\ea\label{ex:prop4}
\avmtmp{
	[\type*{sign}
	phon & \upshape list !(\textit{phon})!\\
	synsem & synsem ]
}
\z
\todosatz{“list” is in textit in \ref{ex:prop6} tho}


The descriptions of specific signs will obviously have specific values for the two features. For example, we might have the following simplified AVM for the phrase \emph{the cat}:

\ea\label{ex:prop5}
\avmtmp{
	[\type*{phrase}
	phon & <\upshape the,cat>\\
	synsem & \upshape NP ]
}
\z


Here, following a widespread practice, we use standard orthography instead of real \emph{phon} objects%
%
\footnote{See Bird \& Klein 1999, Höhle 2018%
	%
	\inlinetodostefan{both not given in references, are they not supposed to appear there? What section are the others referring to?}
	%
	and de Kuthy (this volume, section~7), Abeillé and Chaves (this volume, section~7).}%
%
, and we use the traditional label NP as an abbreviation for the relevant \emph{synsem} object. We will say more about \emph{synsem} objects shortly. First, however, we must say something about phrases.

A central feature of phrases is that they have internal constituents. More precisely, they have daughters, i.e. immediate constituents, one of which may be the head. This information is encoded by further features, for \citet[29]{GSag2000a-u} the features \textsc{daughters} (\textsc{dtrs}) and \textsc{head-daughter} (\textsc{hd-dtr}). The value of the latter is a \emph{sign}, and the value of the former is a list of \emph{signs}, which includes the value of the latter.%
%
\footnote{Some HPSG work, e.g. \citet{Sag97a} has a \textsc{head-daughter} feature and a \textsc{non-head-daughters} feature, and the value of the former is not part of the value of the latter. The sign that is the value of \textsc{hd-dtr} can be a word or a phrase. Within Minimalism, the term ‘head’ is only applied to words. On this usage, the value of \textsc{hd-dtr} is either the head or a phrase containing the head. But there are good reasons for not adopting this usage, for example the fact that the head can be an unheaded phrase for example a coordination (see Abeillé and Chaves, \inlinetodostefan{“chapter” has been lowercase up until now} Chapter~\ref{chap-coordination} of this volume). So we will say that the value of \textsc{hd-dtr} is the head. See \citet[30]{Jackendoff77a} for an early discussion of the term.}
%
{\color{red}
Thus, phrases take the form in (\ref{ex:prop6a}), and headed-phrases the form in (\ref{ex:prop6b}):
}

\begin{multicols}{2}
\ea\label{ex:prop6}
	\ea\label{ex:prop6a}
	\avmtmp{
		[\type*{phrase}
		phon & list!(phon)!\\
		synsem & synsem\\
		dtrs & list!(sign)! ]
	}

\columnbreak
	
	\ex\label{ex:prop6b}
	\avmtmp{
		[\type*{headed-phrase}
		phon & list!(phon)!\\
		synsem & synsem\\
		dtrs & list!(sign)!\\
		hd-dtr & sign ]
	}
	\z
\z
\end{multicols}

To take a concrete example the phrase \emph{the cat} might have the fuller AVM in (\ref{ex:prop7}).

\ea\label{ex:prop7}
\avmtmp{
	[\type*{phrase}
	phon & <\textnormal{the,cat}>\\
	synsem & \textnormal{NP}\\
	dtrs &	<[phon & <\textnormal{the}>\\
			synsem & \textnormal{Det}],
%			
			\1 [phon & <\textnormal{cat}>\\
			synsem & \textnormal{N} ] >\\
	hd-dtr & \1 ]
}
\z

Here the two instances of the tag \avmtmp{\1} indicate that the \emph{synsem} object which is the second member of the \textsc{dtrs} list is also the value of \textsc{hd-dtr}. Thus, the word \emph{cat} is the head of the phrase \emph{the cat}. An object occupying more than one position in a representation, either as a feature value or as part of a feature value, for example \avmtmp{\1} in (\ref{ex:prop7}), is known as re-entrancy or structure-sharing. As we will see below, it is a pervasive feature of HPSG.

Most HPSG work on morphology has assumed a realizational approach, in which there are no morphemes (see Crysmann, this volume, chapter~\ref{chap-morphology}). Hence, words do not have internal structures in the way that phrases do. However, it is widely assumed that lexemes and words that are derived through a lexical rule have the lexeme from which they are derived as a daughter (see Koenig 1999 and below section~4.3).\todosatz{not in references; add sec ref} Hence, the \textsc{dtrs} feature is relevant to words as well as phrases.

AVMs like (\ref{ex:prop7}) can be quite hard to look at. Hence, it is common to use traditional tree diagrams instead. Thus, we might have a tree-like representation in Figure~\ref{fig:prop2} instead of (\ref{ex:prop7}). But one should bear in mind that AVMs correspond to (rooted) graphs and provide richer descriptions than traditional phrase structure trees, with richer node labels and edge labels, and with shared feature values between nodes. Thus, at each node, all kinds of information are available, not just syntax but also semantics and phonology.%
%
\footnote{This differs from Lexical Functional Grammar for instance, which distributes the information between different kinds of structures (see Wechsler and Asudeh, this volume, chapter~\ref{chap-lfg}).}
%

\begin{figure}[h!]
\begin{forest}
[NP
	[Det
		[the]
	]
	[N, edge label={node[midway,right]{\textsc{hd-dtr}}}
		[cat]
	]
]
\end{forest}
	
\caption{A simple tree for \emph{the cat}}\label{fig:prop2}
\end{figure}

If the head is either obvious or unimportant, the \textsc{hd-dtr} annotation might be omitted. This is a convenient informal notation, but it is important to remember that it is just that and has no status within the theory.

We return now to \emph{synsem} objects. Standardly these have two features: \textsc{local}, whose value is a \emph{local} object and \textsc{nonlocal}, which we will deal with in section~5.\todosatz{add sec ref} A \emph{local} object has the features \textsc{cat(egory)} and \textsc{cont(ent)}, whose values are objects of type \emph{category} and \emph{content}, respectively, and the feature \textsc{context}.%
%
\footnote{Words also have a \textsc{morph} attribute that we ignore here (see Crysmann, this volume, chapter~\ref{chap-morphology}).}
%
In much work, a \emph{category} object has the features, \textsc{head}, \textsc{subj} and \textsc{comp(lement)s}. \textsc{head} takes as its value a \emph{part-of-speech} object, while \textsc{subj} and \textsc{comps} have a list of \emph{synsem} objects as their value. The former indicates what sort of subject a sign requires and the latter indicates what complements it takes. In both cases the value is the empty list if nothing is required.  It is generally assumed that the \textsc{subj} list never has more than one member. \textsc{subj} and \textsc{comps} are often called \emph{valence} features. Thus, the following AVM provides a fuller representation of signs:

\ea\label{ex:prop8}
\avmtmp{
	[\type*{sign}
	phon & list!(phon)!\\
	synsem &	[\type*{synsem}
				local &	[\type*{local}
						category &	[\type*{category}
									head & part-of-speech\\
									subj & list!(synsem)!\\
									comps & list!(synsem)!]\\
						content\\
						context]\\
				nonlocal\ldots]
	]
}
\z

The type \emph{part-of-speech} has subtypes such as \emph{noun}, \emph{verb}, and \emph{adjective}. In other words, we have a type hierarchy of the following form:

\begin{figure}[h!]
	\itshape
\begin{forest}
[part-of-speech
	[noun]
	[verb]
	[adjective]
	[\ldots]
]
\end{forest}	
\caption{A hierarchy for part of speech}\label{fig:prop3}
\end{figure}

The type hierarchy can be viewed as an ontology of possible objects in the language. A particular word or phrase must instantiate one of the maximal (most specific) types and have the properties specified for it and all its supertypes.%
%
\footnote{AVMs associated with types used to be combined by type unification \citep{ps}. See Richter, this volume, chapter~\ref{chap-introduction}.}
%
We might have a \emph{synsem} object of the following form for the phrase \emph{the cat}:

\ea\label{ex:prop9}
\avmtmp{
	[\type*{synsem}
	local &	[\type*{local}
			category &	[\type*{category}
						head & noun\\
						subj & <>\\
						comps & <>]\\
			content\\
			context]\\
	nonlocal\ldots]
}
\z

This ignores a number of matters including the value of \textsc{content, context}, and \textsc{nonlocal}. It also ignores the fact that the type \emph{noun} will have certain features, for example \textsc{case}, but it highlights some important aspects of HPSG analyses. Notice that (\ref{ex:prop9}) is compatible with the \textsc{synsem} feature in (\ref{ex:prop8}): it contains more specific information, such as \textsc{head} noun, but no conflicting information: $\langle \rangle$ is the empty list, and is compatible with \emph{list(synsem)}.

Rather different from most of the features mentioned above are fairly traditional features like \textsc{person, number, gender}, and \textsc{case}. In most HPSG work, these have as their value an atomic type, a type with no features. A simple treatment of person might have the types \emph{first, second}, and \emph{third}, and a simple treatment of number the types \emph{sing(ular)} and \emph{plur(al)}.%
%
\footnote{In practice a more complex system of values may well be appropriate \citep{Flickinger2000a}.}
%
There are also Boolean features with + and – as their values. An example is \textsc{aux} used to distinguish auxiliary verbs ([\textsc{aux}~+]) from non-auxiliary verbs ([\textsc{aux}~–]).%
%
\footnote{In some recent work, e.g. \citet[157--162]{Sag2012a}, \citet{Sag2020a}, the feature is used to distinguish positions that only allow an auxiliary from positions that allow any verb. Within this approach auxiliaries (except support do) are unspecified for \textsc{aux} since they may appear in both [\textsc{aux}~+] and [\textsc{aux}~–] constructions. Non-auxiliary verbs are [\textsc{aux}~–], see Abeillé, this volume, chapter~\ref{chap-control-raising}.}
%

As the preceding makes clear, features in HPSG can have a number of kinds of value. They may have an atomic type (\textsc{person, number, gender, case, aux}), a feature structure (\textsc{synsem, local, category}, etc.), or a list of some kind (\textsc{subj, comps}).%
%
\footnote{A list can be analysed as a type of feature structure with the features \textsc{first} and \textsc{rest}, where the value of \textsc{first} is the first element of the list.}
%
As we will see in section~5\todosatz{add sec ref}, HPSG also assumes features with a set as their value.

The \textsc{content} feature, whose value is a \emph{content} object, highlights the importance of semantics within HPSG. But what exactly is a \emph{content} object? Different views of semantics have been taken within the HPSG literature. Much HPSG work has assumed some version of Situation Semantics \citep{BP83a}. But some work has employed so-called Minimal Recursion Semantics \citep{CFPS2005a}, while some others use Logical Resource Semantics \citep{RichterandSailer2001}\todosatz{ref from hpsg-handbook-bibliography.bib!}. \citet[501]{Sag2010b} adopts a conventional, Montague-style possible-worlds semantics in his analysis of English filler-gap constructions and SBCG (section~7.2)\todosatz{add secref} has generally employed a version of Frame Semantics. See Koenig and Richter, this volume, chapter~\ref{chap-semantics} for a discussion of the issues.

Finally, the \textsc{context} feature is used for information structure, deixis, and more generally pragmatics (see de Kuthy, this volume, chapter~\ref{chap-information-structure}).

We will say more about types and features in the following sections. We turn now to constraints. These are implicational statements, saying that if a linguistic object has some property or properties then it must have some other property or properties. They take the following form:%
%
\footnote{The double-shafted arrow ($\Rightarrow$) is used in constraints, and a single shafted arrow |$\rightarrow$ in lexical rules.}
%

\ea\label{ex:prop10}
X $\Rightarrow$ Y
\z

Commonly X is a type and Y a feature description, and this is the case in all the constraints that we discuss below. However, X may also be a feature description with or without an associated type. This is necessary, for example, in the constraints that constitute binding theory. See Müller and Branco, this volume, chapter~\ref{chap-binding}. Here is a very simple constraint:

\ea\label{ex:prop11}
\emph{phrase} $\Rightarrow$ [\textsc{comps} $\langle \rangle$]
\z

This says that a phrase has the empty list ($\langle \rangle$) as the value of the \textsc{comps} feature, which means that it does not require any complements.%
%
\footnote{The constraint in (\ref{ex:prop11}) is plausible for English, but it is too strong for some languages, especially for languages with complex predicates or partial VPs (see Godard and Samvelian, this volume, chapter~1),\inlinetodostefan{what ch is this actually referring to??} \textcolor{red}{and also for SOV languages if they are analysed in terms of binary branching (see Müller, this volume, chapter~\ref{chap-order}).}}
%
As we will see below, most constraints are more complex than (\ref{ex:prop11}) and impose a number of restrictions on certain objects. For this reason, one might speak of a set of constraints. However, we will continue to use the term constraint for objects of the form in (\ref{ex:prop10}), no matter how many restrictions are imposed. Particularly important are constraints dealing with internal structure of various types of phrase. We will consider some constraints of this kind in section~5.\todosatz{add secref}

In most HPSG work, some shortcuts are used to abbreviate a feature path, for example in (\ref{ex:prop11}), \textsc{comps} stands for \textsc{synsem|loc|cat|comps}. We use this practice in the rest of the chapter and \textcolor{red}{it is used} throughout the Handbook.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{The lexicon}

As noted above, the type \emph{lexical-sign}, its subtypes, and the constraints on them are central to the lexicon of a language and the words it licenses.%
%
\footnote{Other types of constraint are relevant to the form of lexemes and words, e.g. constraints on \emph{synsem} objects and constraints on \textsc{phon} values. These are also relevant to the form of phrases.}
%
Lexical rules are also important. Some of the earliest work in HPSG focused on the organization of the lexicon and the question of how lexical generalizations can be captured, and detailed proposals have been developed.%
%
\footnote{The lexicon is more important in HPSG than in some other constructional approaches, e.g. that of \citet{Goldberg95a,Goldberg2006a}. See \citet{MWArgSt} and Müller, this volume, chapter~\ref{chap-cxg} for discussion.}
%

%%%%%%%%%%%%%%%%%%%%
\subsection{Lexemes and words}

In some frameworks, the lexicon contains not lexemes but morphemes, i.e. roots and affixes of various kinds. But most work in HPSG has assumed a realizational approach to morphology. Within this approach, there are no morphemes, just lexemes and the words that realize them, and affixes are just bits of phonology realizing certain morphosyntactic features \citep{Stump2001a-u-kopiert,Anderson92a-u}. One consequence of this is that HPSG has no syntactic elements like the T(ense) and Num(ber) functional heads of Minimalism, which are mainly realized by affixes. See Crysmann, this volume, chapter~\ref{chap-morphology} and Davis and Koenig, this volume, chapter~\ref{chap-lexicon} for discussion.

Probably the most important properties of any lexeme are its part of speech and its combinatorial properties. As we saw in the last section, the \textsc{head} feature encodes part of speech information while the \textsc{subj} and \textsc{comps} features encode combinatorial information. As we also noted in the last section, \textsc{head} takes as its value a \emph{part-of-speech} object and the type \emph{part-of-speech} has subtypes such as \emph{noun}, \emph{verb}, and \emph{adjective}. At least some of the subtypes have certain features. For example, in many languages the type noun has the feature \textsc{case} with values like \emph{nom(inative), acc(usative)}, and \emph{gen(itive)}. Thus, nominative pronouns like \emph{I} might have a \emph{part-of-speech} of the form in (\ref{ex:prop12}) as its \textsc{head} value.

\ea\label{ex:prop12}
\avmtmp{
	[\type*{noun}
	case & nom]
}
\z

Similarly, in many languages the type verb has the feature \textsc{vform} with values like \emph{fin(ite)} and \emph{inf(initive)}. Thus, the \emph{part-of-speech} of the word form \emph{be} might be (\ref{ex:prop13}).

\ea\label{ex:prop13}
\avmtmp{
	[\type*{verb}
	vform & inf]
}
\z

In much the same way, the type \emph{adjective} will have a feature distinguishing between positive, comparative, and superlative forms, in English and many other languages.

We must now say more about combinatorial properties. In much HPSG work it is assumed that \textsc{subj} and \textsc{comps} encode what might be regarded as superficial combinatorial information and more basic combinatorial information is encoded by a feature \textsc{arg(ument)-st(ructure)}.%
%
\footnote{\textsc{arg-st} is also crucial for binding theory, which takes the form of a number of constraints on \textsc{arg-st} lists. See Müller and Branco, this volume, chapter~\ref{chap-binding}.}
%
Normally the value of \textsc{arg-st} of a word is the concatenation of the values of \textsc{subj} and \textsc{comps}, using $\oplus$ for list concatenation. In other words, we normally have the following situation (notice the use of re-entrancy or structure-sharing):

\ea\label{ex:prop14}
\avmtmp{
	[subj & \1\\
	comps & \2\\
	arg-st & \1 \+ \2]
}
\z

As noted earlier, it is generally assumed that the \textsc{subj} list never has more than one member. The appropriate features for the word \emph{read} in (\ref{ex:prop1a}) for example would include the following, where the tags identify not lists but list members:

\ea\label{ex:prop15}
\avmtmp{
	[subj & <\1>\\
	comps & <\2>\\
	arg-st & <\1 \upshape NP, \2 NP>]
}
\z

Under some circumstances, however, we have something different. For example, it has been proposed, e.g. in \citet[65]{ManningandSag1998},\todosatz{ref from hpsg-handbook-bib} that null subject sentences have an element representing the understood subject in the \textsc{arg-st} list of the main verb but nothing in the \textsc{subj} list. Thus, the verb \emph{czytałem} in (\ref{ex:prop1b}), repeated here as (\ref{ex:prop16}), has the features in (\ref{ex:prop17}).

\ea\label{ex:prop16}
\gll Czytałem książkę.\\
write.\textsc{pst.1sg} book.\textsc{acc}\\
\glt ‘I read a book.’

\ex\label{ex:prop17}
\avmtmp{
	[subj & <>\\
	comps & <\1>\\
	arg-st & <\upshape NP, \1NP>]
}
\z

A similar analysis is widely assumed for unbounded dependency gaps. On this analysis, the verb \emph{say} in (\ref{ex:prop2}), repeated here as (\ref{ex:prop18}), has the features in (\ref{ex:prop19}).

\ea\label{ex:prop18}
What did you say?

\ex\label{ex:prop19}
\avmtmp{
	[subj & <\1 \upshape NP>\\
	comps & <>\\
	arg-st & <\1 NP, NP>]
}
\z

It is also assumed that the arguments that are realised \textcolor{red}{as pronominal affixes (traditionally known as clitics in Romance languages)} are absent from \textsc{comps} lists \citep{MS97a-u,monachesi05},\todosatz{Monachesi from hpsg-hb.bib} and other differences between \textsc{subj} and \textsc{comps} and \textsc{arg-st} have been proposed for other languages (see \citealt{ManningandSag1998}, Wechsler, Koenig and Davies,\todosatz{from hpsg-bib} this volume, chapter~\ref{chap-arg-st} for discussion. In much work, the relation \textsc{arg-st} and \textsc{subj} and \textsc{comps} is regulated by a constraint called the Argument Realisation Principle (ARP). The following is a simplified version of the constraint proposed in \citet[171]{GSag2000a-u} (see also \citealt[12]{BMS2001a}):

\ea\label{ex:prop20}
\emph{word} \impl
\avmtmp{
	[subj & \1\\
	comps & \2 \- \textnormal{list}!(non-canonical)!\\
	arg-st & \1 \+ \2]
}
\z

This ensures that non-canonical arguments, including gaps and arguments realized as a clitics do not appear in \textcolor{red}{\textsc{comps} lists}.%
%
\footnote{The sign $\oplus$ means\ concatenation\ of\ lists.\ In A $\ominus$ B=C, $\ominus$ stands for ‘a relation where C is equal to A, iff B is the empty list. Otherwise C is the list that deletes the first part in A that is identical to B’ (Müller 2000:258)\inlinetodostefan{not in references}}
%
Notice, however, that it says nothing special about subjects and so does not allow an AVM like (\ref{ex:prop17}).%
%
\footnote{\citet[177--183]{GSag2000a-u} explicitly allow gaps in \textsc{subj} lists, but this is controversial, as discussed in Borsley and Crysmann, this volume, chapter~\ref{chap-udc}.}
%
There are complex issues here, and the Principle will probably take a different form in different languages. So we will not try decide exactly what form it should take.

A variety of HPSG work assumes the \textsc{subj} and \textsc{comps} features, but some work assumes a \textsc{spr (specifier)} feature instead of or in addition to the \textsc{subj} feature. Where it replaces \textsc{subj}, the idea is that subjects are one of a number of types of specifiers, others being determiners within NPs and degree words like \emph{so} and \emph{too} within APs \citep{SWB2003a}. Where it is an additional feature, the idea is that there are number of types of specifier but subjects are not specifiers. Predicative nominals (e.g. \emph{my cousin} in \emph{Paul is my cousin}) may need both (\citealp[9.4.1]{ps2}; \citealp[409]{GSag2000a-u}\citealp{AG2003a-b}). There are other positions in the HPSG community. Much early work has a single feature called \textsc{subcat} instead of \textsc{subj} and \textsc{comps} \citep{ps}. Essentially the same position has been adopted within Sign Based Construction Grammar, which has a single feature called \textsc{valence} instead of \textsc{subj, spr} and \textsc{comps}.%
%
\footnote{SBCG also has a feature \textsc{x-arg}, which picks out subjects and other external arguments. But unlike the other features mentioned here, this always has the same value in a head and its mother. Its role is to make information about external arguments available outside the phrases in which they appear.  See \citet[84, 149--151]{Sag2007a,Sag2012a}.}
%
Obviously, there are some important issues here.

It is an important feature of lexical items that part of speech and combinatorial properties are separate matters. Members of the same part of speech can have different combinatorial properties and members of different parts of speech can have the same combinatorial properties. Much HPSG work captures this fact by proposing that the type \emph{lexeme} be cross-classified along two dimensions, one dealing with part-of-speech information and one dealing with argument selection information \citep{Flickinger87}. Figure~\ref{fig:prop4} is a simple illustration based on \citet[20]{GSag2000a-u}:

\begin{figure}[h!]
\begin{forest}
[\emph{lexeme},s sep=2cm
	[\textsc{part-of-speech},draw
		[\emph{v-lx},name=v]
		[\ldots]
		[\ldots]
		[\ldots]
	]
	[\textsc{arg-selection},draw
		[\emph{intr-lx}
			[\emph{s-rsg-lx}
				[\emph{srv-lx},name=srv]
				[,phantom]
			]
			[\ldots]
			[\ldots]
		]
		[\ldots]
	]
]
{
	\draw (v.south) to (srv.north);
}
\end{forest}
\caption{Cross-classification of lexemes}\label{fig:prop4}
\end{figure}

Upper case letters are used for the two dimensions of classification, and \emph{v-lx, intr-lx, s-rsg-lx}, and \emph{srv-lx} abbreviate \emph{verb-lexeme, intransitive-lexeme, subject-raising-lexeme}, and \emph{subject-raising-verb-lexeme}, respectively. All these types will be subject to specific constraints, For example, \emph{v-lx} will be subject to something like the following constraint, based on that in \citet[22]{GSag2000a-u}:

\ea\label{ex:prop21}
\emph{v-lx} \impl
\avmtmp{
	[head & verb\\
	arg-st & <\upshape XP \ldots>]
}
\z

This says that a verb lexeme has a \emph{verb} part of speech and requires a phrase of some kind as its first (syntactic) argument (corresponding to its subject). Similarly, we will have something like the following constraint for \emph{s-rsg-lx}:

\ea\label{ex:prop22}
\emph{s-rsg-lx} \impl
\avmtmp{
	[arg-st & <\1, [subj & <\1>] \ldots>]
}
\z

This says that a subject-raising-lexeme has (at least) two (syntactic) arguments, a subject and a complement, and that the subject is whatever the complement requires as a subject, indicated by \avmtmp{\1}. Most of the properties of any lexeme will be inherited from its supertypes. Thus, very little information needs to be listed for each specific lexeme, and the richness of the lexical description comes from the classification in a system like this.

For example, for a subject-raising verb like \emph{seem}, its \textcolor{red}{\textsc{cat}} and \textsc{content} features are the following, using Minimal Recursion semantics: \textsc{rels} is the attribute for the list of elementary predications associated with a word, a lexeme or a phrase, \textsc{soa} is for state-of-affair (see Koenig and Richter, this volume, chapter~\ref{chap-semantics}). \emph{Seem} takes an infinitival VP complement.%
%
\footnote{The entry can be modified to allow predicative complements, as well as a second \emph{to} complement (\emph{John seems tired/ in a good mood to me}).}
%
Notice that no semantic role is assigned to the first argument (see Abeillé, this volume, chapter~\ref{chap-control-raising}).

\ea\label{ex:prop23}
\type{seem-lx} \impl \type{s-rsg-lx} \&
\avmtmp{
	[cat & [arg-st & <[], \textnormal{VP}	[head & [vform & inf]\\
											index & s1]>]\\
	cont &	[index & s\\
			rels & <[\type*{seems-rel}
					soa & s1]>]
	]
}
\z

Once these more specific features are combined with features from type \emph{s-rsg-lxm}, we get a more complete AVM like the following for the word \emph{seem}:

\ea\label{ex:prop24}
\emph{seem-lx} \impl
\avmtmp{
	[cat &	[arg-st & <\1, \2 \textnormal{VP}	[head & [vform & inf]\\
												subj & <\1>\\
												index & s1]>\\
			subj & <\1>\\
			comps & <\2>]\\
%			
	cont &	[index & s\\
			rels & <[\type*{seems-rel}
					soa & s1]>
			]
	]
}
\z

Notice that the \textsc{subj} feature is underspecified. Thus, \emph{seem} combines with an infinitival complement and with any subject (nominal or verbal, expletive or referential), provided this subject is appropriate for its complement (see Abeillé, this volume, chapter~\ref{chap-control-raising}):

\ea\label{ex:prop25}
	\ea John /*Working is sleeping.
	\ex John /* Working seems to be sleeping.
	\ex Working is/ seems to be tiring
	\ex It is raining / seems to be raining.
	\z
\z

%%%%%%%%%%%%%%%%%%%%
\subsection{Lexical rules}

The hierarchy of lexical types provides one way of capturing lexical generalizations. Lexical rules provide another.%
%
\footnote{Lexical rules can be seen as a generative device, or alternatively, as a set of well-formedness conditions on the lexicon: if the lexicon contains items with description x, it must also contain items with description y \citep{Meurers2001a}. See Davis and Koenig, this volume chapter~\ref{chap-lexicon}.}
%
They are used in morphology to relate lexemes to words (inflection) and lexemes to lexemes (derivation) (see Crysmann, this volume, chapter~\ref{chap-morphology}). For syntax, they are relevant especially to valence alternations such as that illustrated in the following (see Wechsler, Koenig and Davis, this volume, chapter~ \ref{chap-arg-st}):

\ea\label{ex:prop26}
	\ea That Kim was late annoyed Lee.
	\ex That Sandy was there is unimportant. 
	\ex That Lee won impressed everyone.
	\z
	
\ex\label{ex:prop27}
	\ea It annoyed Lee that Kim was late.
	\ex It is unimportant that Sandy was there. 
	\ex It impressed everyone that Lee won.
	\z
\z

These show that verbs and adjectives which allow a clausal subject generally also allow an expletive \emph{it} subject and a clause as an extra complement \citep[150]{ps2}. The lexemes required for the latter use can be derived from the lexemes required for the former use by a lexical rule of the following form:%
%
\footnote{Another representation of lexical rules is an AVM with features \textsc{input} and \textsc{output}, or with the left hand side as a daughter. As for (\ref{ex:prop27}), assuming both clauses and VPs have a verbal head, it easily extends to infinitival subjects, to accommodate pairs of examples like the following:
	\ea To annoy Lee is easy.
	\ex It is easy to annoy Lee.
	\z
Clauses introduced by \emph{that} are sometimes considered as CPs in HPSG (see section~7),\inlinetodostefan{add secref} with verbs and complementizers as two subtypes of \emph{verbal}.
	
}
%

\ea\label{ex:prop28}
\avmtmp{[arg-st & <S> \+ \2] $\mapsto$
	[arg-st & <NP![\type{it}]!> \+ \2 \+ <S>]}
\z

The active-passive relation can be captured by a similar lexical rule \citep{Flickinger87}. Since these rules do not change the \textsc{content} feature, these alternations will preserve the meaning of the verb or adjective lexeme (see Davis and Koenig, this volume, chapter~ \ref{chap-lexicon}). Thus, the sentences in (\ref{ex:prop27}) will have a different syntactic structure from their counterparts in (\ref{ex:prop26}) but may have the same semantic representation (they will probably have different information structures, thus different \textsc{context} features); see de Kuthy, this volume, chapter~\ref{chap-information-structure}.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\section{Syntax}

As noted above, the type \emph{phrase}, its subtypes, and the constraints on them are at the heart of the syntax of a language.%
%
\footnote{As noted in fn.18, constraints on \emph{synsem} objects and \textsc{phon} values are relevant to phrases as they are to lexemes and words.}
%
A simple hierarchy of phrase types was assumed in early HPSG, but what we have called Construction-based HPSG employs complex hierarchies of phrase types comparable to the complex hierarchies of lexical types employed in the lexicon.

%%%%%%%%%%%%%%%%%%%%
\subsection{A hierarchy of phrase types}

Like much other work in syntax, HPSG takes from X-bar theory \citep{Jackendoff77a} the idea that the local trees that make up syntactic structures fall into a limited number of types. Like \citet{Jackendoff77a}, and unlike Minimalism, HPSG assumes that not all phrases are headed, even if many are, and does not limit the term ‘head’ to lexical elements. Thus, among phrases there is a basic distinction between non-headed-phrases and headed-phrases. There are various kinds of headed phrase. We will consider three here. First there are head-complement-phrases, combinations of a head and its complements. These can be headed by various parts of speech, verbs, prepositions, adjectives, nouns, and others, and may have one complement or more than one. Next, there are head-subject-phrases. Typically, the head of such a phrase is a VP. However, the bracketed material in the following may well be head-subject-phrases with a non-verbal head. 

\ea\label{ex:prop29}
With [Kim ill/in London/a candidate], anything is possible.
\z

Finally, there are head-filler-phrases, clauses in which an initial constituent is associated with a gap in the following constituent. \emph{Wh}-interrogatives and \emph{wh}-relatives, such as the bracketed material in the following, are typical examples.

\ea\label{ex:prop30}
	\ea I’m wondering [who I talked to].
	\ex This is the official [who I talked to].
	\z
\z

All this suggests the simple type hierarchy in figure~\ref{fig:prop5}:

\begin{figure}[h!]
	\itshape
\begin{forest}
[phrase
	[non-headed-ph]
	[headed-ph
		[hd-comp-ph]
		[hd-subj-ph]
		[hd-filler-ph]
	]
]
\end{forest}
\caption{A Hierarchy of types of phrases}\label{fig:prop5}
\end{figure}

Each of these types is associated with a constraint capturing its distinctive properties.

Consider first the type \emph{hd-ph}. Here we need a constraint capturing what all headed-phrases have in common. This is essentially that they have a head, with which they share certain features. But what features? One view is that the main features that are shared are those that are the value of \textsc{head}. This is embodied in the following constraint, which is known as the Head Feature Principle:

\ea\label{ex:prop31}
\type{headed-ph} \impl
\avmtmp{
	[head & \1\\
	head-dtr & [head & \1]]
}
\z

Each of the three subtypes of \emph{headed-ph} is subject to a constraint embodying its distinctive properties. Here is a constraint on the type \emph{hd-comp-ph} (with \textsc{synsem} abbreviated as \textsc{ss}):

\ea\label{ex:prop32}
\emph{hd-comp-ph} \impl
\avmtmp{
	[hd-dtr & \1	[\type*{word}
					comps & <\2, \ldots,\tag{\emph{n}}>]\\
	dtrs & <\1, [ss & \2], \ldots, [ss & \tag{\emph{n}}]>]
}
\z

This ensures that a head-complement-phrase has a word as a head daughter and non-head daughters with the \emph{synsem} properties that appear in the head’s \textsc{comps} list.%
%
\footnote{The head could be identified as a [\textsc{lex}~+], [\textsc{light}~+], or [\textsc{weight}~\emph{light}] phrase, to accomodate coordination of heads as in \emph{John [knows and likes] this record.} \citep{Abeille2006a}.}
%
Notice that nothing is said about the \textsc{synsem} value of the phrase. It will be [\textsc{comps}~<>], as required by the constraint in (\ref{ex:prop14}), and it will have the same value for \textsc{head} as the Head-daughter as a consequence of the Head-Feature-Principle. It must also have the same value for \textsc{subj} as the head daughter. One might add this to the constraint in (\ref{ex:prop31}), but that would miss a generalization. Head-complement-phrases are not the only phrases which have the same value for \textsc{subj} as their head. This is also a feature of head-filler-phrases, as we will see below. It seems in fact that it is normal for a phrase to have the same value for any Valence feature as its head. This is often attributed to the Valence Principle, which can be stated informally as follows (cf. Sag and Wasow 1999: 86):\todosatz{not in references}

\ea\label{ex:prop33}
Unless some constraint says otherwise, the mother’s values for the Valence features are identical to those of the Head daughter.
\z

There is no assumption in HPSG that all branching is binary.%
%
\footnote{Implemented HPSG grammars for various languages may assume binary branching structures \citep{Flickinger2000a,Copestake2002a,MuellerCoreGram}, see Bender this volume.}
%
Hence, where a head takes two complements, both may be its sisters. An example of the sort of structures that the analysis licenses is illustrated in Figure~\ref{fig:prop6}.

\begin{figure}[h!]
\begin{forest}sm edges
[\avmtmp{
	[\type*{hd-comp-ph}
	head & \1 verb\\
	subj & \2 <NP>\\
	comps & <>]
},l sep=1cm
	[\avmtmp{
		[\type*{word}
		head & \1\\
		subj & \2\\
		comps & <\3, \4>]
	}, edge label={node[midway,left]{\textsc{hd-dtr~~}}}
		[give]
	]
	[\avmtmp{\3 NP}
		[some money,roof]
	]
	[\avmtmp{\4 PP}
		[to charity,roof]
	]
]
\end{forest}
\caption{A tree for a head-complement phrase}\label{fig:prop6}
\end{figure}

Instead of the Head Feature Principle and the Valence Principle, \citet[33]{GSag2000a-u} propose the Generalized Head Feature Principle, which takes the following form:

\ea\label{ex:prop34}
\type{headed-ph} \impl
\avmtmp{
	[synsem & / \1\\
	hd-dtr & [synsem & / \1]]
}
\z

The slashes (/) here indicate that this is a default constraint \citep{LC99a}. Thus, it says that a headed-phrase and its head daughter have the same \textsc{synsem} value unless some other constraint requires something different. In versions of HPSG which assume this constraint, it is responsible among many other things for the fact that a head-complement-phrase has the same value for \textsc{subj} as the head daughter.

We turn now to the type \emph{hd-subj-ph}. Here we need a constraint which mentions the \textsc{synsem} value of the phrase, and not just the daughters, as follows:

\ea\label{ex:prop35}
\type{hd-subj-ph} \impl
\avmtmp{
	[subj & <>\\
	hd-dtr & \1	[comps & <>\\
				subj & <\2>]\\
	dtrs & <[synsem \2], \1>]
}
\z

This ensures that a head-subject-phrase is [\textsc{subj}~<>] and has a head daughter which is [\textsc{comps}~<>], and a non-head daughter with the \emph{synsem} properties that appear in the head’s \textsc{subj} list.%
%
\footnote{Instead of requiring the head to be [\textsc{comps}~<>], one might require it to be a phrase (which would be required by (\ref{ex:prop11}) to be [\textsc{comps}~<>]). However, this would require e.g. \emph{laughed} in \emph{Kim laughed} to be analysed as a phrase consisting of a single word. With (\ref{ex:prop34}) it can be analysed as just a word.}
%
It licenses structures like that in Figure~\ref{fig:prop7}.

\begin{figure}[h!]
\begin{forest}sm edges
[\avmtmp{
	[\type*{hd-subj-ph}
	head & \1 verb\\
	subj & <>\\
	comps & <>]
},l sep=1cm
	[\avmtmp{\2 NP}
		[They]
	]
	[\avmtmp{
		[\type*{hd-comp-ph}
		head & \1\\
		subj & <\2>\\
		comps & <>]
	}, edge label={node[midway,right]{\textsc{~hd-dtr}}},l sep=1cm
		[give some money to charity,roof]
	]
]
\end{forest}
\caption{A tree for a head-subject phrase}\label{fig:prop7}
\end{figure}

Finally, we consider the type \emph{hd-fill-ph}. This involves the feature \textsc{slash}, one of the features contained in the value of the feature \textsc{nonlocal} introduced earlier in (\ref{ex:prop9}). Its value is a set of local feature structures and it encodes information about unbounded dependency gaps (see Borsley \& Crysmann, this volume, chapter~\ref{chap-udc}). Here is the relevant constraint:%
%
\footnote{We use $\cup$ for set union. Notice that the root category does not have to have an empty \textsc{slash} list, thus allowing for multiple extractions (\emph{Paul, who could we talk to about?}).}
%

\ea\label{ex:prop36}
\type{hd-filler-ph} \impl
\avmtmp{
	[slash & \1\\
	hd-dtr & \2	[comps & <>\\
				slash & \{\3\} $\cup$ \1]\\
	dtrs & <[local & \3], \2>]
}
\z

This says that a head-filler-phrase has a head daughter, with a \textsc{slash} set which is the \textsc{slash} set of the head-filler-phrase plus one other \emph{local} feature structure, and a non-head daughter, whose \textsc{local} value is the additional \emph{local} feature structure of the head daughter. \avmtmp{\1} is normally the empty set.%
%
\footnote{As with (\ref{ex:prop34}), one might substitute phrase here for [\textsc{comps}~<>]. But this would mean that \emph{to} in \emph{I would do it but I don’t know how to} must be analysed as a phrase containing a single word. With (\ref{ex:prop36}) it can be just a word.}
%
Figure~\ref{fig:prop8} illustrates a typical head-filler-phrase.

\begin{figure}[h!]
\begin{forest}sm edges
[\avmtmp{
	[\type*{hd-fill-ph}
	head & \1 verb\\
	subj & <>\\
	comps & <>\\
	slash & \{\}]
},l sep=1cm
	[\avmtmp{[local & \2 \upshape NP]}
		[who]
	]
	[\avmtmp{
		[\type*{hd-subj-ph}
		head & \1\\
		subj & <>\\
		comps & <>\\
		slash & \{\2\}]
	}, edge label={node[midway,right]{\textsc{~hd-dtr}}},l sep=1cm
		[I talked to,roof]
	]
]
\end{forest}
\caption{A tree for a head-filler phrase}\label{fig:prop8}
\end{figure}

Notice that the head daughter in a head-filler-phrase is not required to have an empty \textsc{subj} list (it is not marked as [\textsc{subj}~<>]) and hence does not have to be a head-subject-phrase. It can also be a head-complement-phrase (a VP), as in the following:

\ea\label{ex:prop37}
I’m wondering [who [to talk to]].
\z

Either the Valence Principle or the Generalized Head Feature Principle will ensure that a head-filler-phrase has the same value for \textsc{subj} as its head daughter.

The constraints that we have just discussed are rather like phrase structure rules. This led \citet[33]{GSag2000a-u} to use an informal notation which reflects this. This involves the phrase type on the first line followed by a colon, and information about the phrase itself and its daughters on the second line separated by an arrow and with the head daughter identified by ‘\textbf{H}’. Thus, instead of (\ref{ex:prop38a}) one has (\ref{ex:prop38b}).

\ea\label{ex:prop38}
	\ea\label{ex:prop38a}
	\type{phrase} \impl
	\avmtmp{
		[synsem & \upshape X\\
		dtrs & <\1 Y, Z>\\
		hd-dtr & \1]
	}
	\ex\label{ex:prop38b}
	\emph{phrase:}
	
	X $\to$ \textbf{H}[Y], Z
	\z
\z

Notice that while the double arrow in (\ref{ex:prop38a}) has the normal ‘if-then’ interpretation, the single arrow in (\ref{ex:prop38b}) means ‘consists of’. In some circumstances this informal notation may be more convenient than the more formal notation used above.

In the preceding discussion, we have ignored the semantics of the phrase. Leaving aside quantification and other complex matters, the \textsc{content} of a headed phrase can be handled via two semantic principles, assuming \textsc{index} and \textsc{rel}ations as in MRS (26)\todosatz{This is supposed to be 36, right?} above: a coindexing principle (the \textsc{index} of a headed-phrase is the \textsc{index} of its \textsc{head-dtr}) and a ‘compositionality’ principle (the \textsc{rels} of a phrase is the contatenation of the \textsc{rels} of its \textsc{dtrs}) (\citealp{CFPS2005a}; Koenig and Richter, this volume, chapter~\ref{sec-minimal-recursion-semantics}).

The type hierarchy in (\ref{ex:prop34}) is simplified in a number of respects. It includes no non-headed-phrases.%
%
\footnote{The most important type of non-headed phrase is coordinate structure. See Abeillé and Chaves, Chapter~\ref{chap-coordination} of this volume, for discussion.}
%
It also ignores various other subtypes of headed-phrase, some of which are discussed in the next section. Most importantly, it is widely assumed that the type \emph{phrase} like the type \emph{lexeme} can be cross-classified along two dimensions, one dealing with head-dependent relations and the other dealing with the properties of various types of clauses. A simplified illustration is given in Figure~\ref{fig:prop9}.

\begin{figure}[h!]
\begin{forest}
[\emph{phrase}
	[\textsc{headedness},draw
		[\ldots]
		[\emph{headed-phrase}
			[\ldots]
			[\ldots]
			[\emph{head-fill-ph}
				[\ldots]
				[\ldots]
				[\emph{wh-interr-cl},name=2]
			]
		]
	]
	[\textsc{clausality},draw
		[\emph{clause}
			[\emph{interr-cl},name=1]
			[\ldots]
		]
		[\emph{non-clause}]
	]
]
{
\draw (1.south) to (2.north);
}
\end{forest}
\caption{Cross-classification of phrases}\label{fig:prop9}
\end{figure}

Here \emph{wh-interr-cl} is identified as a subtype of \emph{head-fill-ph} and a subtype of \emph{interr(ogative)-cl}. As such, it has both the properties required by the constraint in (\ref{ex:prop42})\todosatz{42 isnt a constraint tho} and certain properties characteristic of interrogative clauses, most obviously interrogative semantics.

%%%%%%%%%%%%%%%%%%%%
\subsection{Constituency and constituent order}

We must now say something about constituent order. In much HPSG work this is a matter of phonology, more precisely a matter of the relation between the \textsc{phon} value of a phrase and the \textsc{phon} values of its daughters.%
%
\footnote{As discussed in section~7.1,\inlinetodostefan{add secref} in some HPSG work, linear order is a property of so-called order domains, which essentially mediate constituent structure and phonology (see Müller, this volume, chapter~\ref{chap-order}). }
%
Consider, for example, a phrase with two daughters, each with its own \textsc{phon} value. The \textsc{phon} value of the phrase will be the concatenation of the \textsc{phon} values of the daughters. Clearly, they can be concatenated in two ways as follows, as in (\ref{ex:prop39}), or their order may be left unspecified for ‘free’ word order’:

\ea\label{ex:prop39}
\avmtmp{
	[phon & \1 \+ \2\\
	dtrs & <[phon \1], [phon \2]>]
}
\avmtmp{
	[phon & \2 \+ \1\\
	dtrs & <[phon \1], [phon \2]>]
}
\z

Within this approach, the following English and Welsh examples might have exactly the same analysis (a head-adjunct phrase) except for their \textsc{phon} values:

\ea\label{ex:prop40}
	\ea\label{ex:prop40a}
	black sheep
	\ex\label{ex:prop40b}
	\gll defaid du\\
	sheep.\textsc{pl} black\\
	\glt ‘black sheep’
	\z
\z


















	











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\caption{Frequencies of word classes}
\label{tab:1:frequencies}
 \begin{tabular}{lllll} % add l for every additional column or remove as necessary
  \lsptoprule
            & nouns & verbs & adjectives & adverbs\\ %table header
  \midrule
  absolute  &   12 &    34  &    23     & 13\\
  relative  &   3.1 &   8.9 &    5.7    & 3.2\\
  \lspbottomrule
 \end{tabular}
\end{table}

\citep{Chomsky57a}.

\citet{Meier2017}
\ea\label{ex:1:descartes}
\langinfo{Latin}{}{personal knowledge}\\
\gll cogit-o ergo sum \\
think-1{\SG}.{\PRS}.{\IND} hence exist.1{\SG}.{\PRS}.{\IND}\\
\glt `I think therefore I am'
\z


\is{prolegomena}
Sed cursus \footnote{eros condimentum mi consectetur, ac consectetur} sapien pulvinar. Sed consequat, magna\footnote{eu scelerisque laoreet, ante erat tristique justo, nec cursus eros diam eu nisl. Vestibulum non arcu tellus}. Nunc dignissim tristique massa ut gravida. Nullam auctor orci gravida tellus egestas, vitae pharetra nisl porttitor. Pellentesque turpis nulla, venenatis id porttitor non, volutpat ut leo. Etiam hendrerit scelerisque luctus. Nam sed egestas est. Suspendisse potenti. Nunc vestibulum nec odio non laoreet. Proin lacinia nulla lectus, eu vehicula erat vehicula sed. 

\section*{Abbreviations}
\begin{tabularx}{.45\textwidth}{lX}
\textsc{cop} & copula\\ 
\textsc{fv} & final vowel\\
\end{tabularx}
\begin{tabularx}{.45\textwidth}{lX}
\textsc{neg} & negation\\ 
\textsc{sm} & subject marker\\
\end{tabularx}


\section*{Acknowledgements}

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
}
\end{document}
